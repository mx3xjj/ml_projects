{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: Use Keras to create a Deep NN to classify hand written numbers #\n",
    "\n",
    "We will be using the MNIST dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load the data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training data\n",
    "'''\n",
    "X_train, y_train = load_mnist('mnist', kind='train')\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Testing data\n",
    "'''\n",
    "X_test, y_test = load_mnist('mnist', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Investigate data\n",
    "'''\n",
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data preprocessing ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Cast the MNIST image array into 32 bit format\n",
    "'''\n",
    "import theano \n",
    "\n",
    "# Define config for float\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "# Cast training and testing data into float32\n",
    "X_train = X_train.astype(theano.config.floatX)\n",
    "X_test = X_test.astype(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('First 3 labels: ', array([5, 0, 4], dtype=uint8))\n",
      "\n",
      "First 3 labels (one-hot):\n",
      "\n",
      "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Convert class labels into one-hot format\n",
    "'''\n",
    "from keras.utils import np_utils\n",
    "import pprint\n",
    "\n",
    "print('First 3 labels: ', y_train[:3])\n",
    "\n",
    "# One hot encode using to_categorical()\n",
    "y_train_ohe = np_utils.to_categorical(y_train) \n",
    "\n",
    "print '\\nFirst 3 labels (one-hot):\\n'\n",
    "pprint.pprint(y_train_ohe[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: Set up Deep NN using Keras##\n",
    "\n",
    "* Set hyperbolic tangent as the activation function.\n",
    "* Set softmax for the output layer classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Deep NN set up using Keras\n",
    "'''\n",
    "# Necessary imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# Feedforward Neural Network\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "# Input layer - input dimension is the number of rows it gets as input, activation function is the tanh function.\n",
    "model.add(Dense(input_dim = X_train.shape[1], \n",
    "                output_dim = 50, \n",
    "                init = 'uniform', \n",
    "                activation = 'tanh'))\n",
    "\n",
    "# Hidden layer\n",
    "# Sam input dimension as output of input layer\n",
    "model.add(Dense(input_dim = 50, \n",
    "                output_dim = 50, \n",
    "                init ='uniform', \n",
    "                activation= 'tanh'))\n",
    "\n",
    "# Output layer\n",
    "# Activation function is the softmax function\n",
    "model.add(Dense(input_dim = 50, \n",
    "                output_dim = y_train_ohe.shape[1], \n",
    "                init = 'uniform', \n",
    "                activation = 'softmax'))\n",
    "\n",
    "# Define optimizer - Stochastic gradient descent\n",
    "sgd = SGD(lr = 0.001, # learning rate\n",
    "          decay = 1e-7, # weight decay constant\n",
    "          momentum = .9) # momentum learning\n",
    "\n",
    "# Set the loss function and compile the model using the optimizer\n",
    "# categorical_crossentropy is the generalization of binary cross entropy for multiclass classification problems.\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "              optimizer = sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.3425 - val_loss: 0.2877\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.3360 - val_loss: 0.2855\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.3217 - val_loss: 0.2701\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.3133 - val_loss: 0.2809\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.3104 - val_loss: 0.2532\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2961 - val_loss: 0.2500\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2959 - val_loss: 0.2488\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2846 - val_loss: 0.2446\n",
      "Epoch 9/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2789 - val_loss: 0.2412\n",
      "Epoch 10/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2758 - val_loss: 0.2426\n",
      "Epoch 11/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2804 - val_loss: 0.2281\n",
      "Epoch 12/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2649 - val_loss: 0.2231\n",
      "Epoch 13/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2569 - val_loss: 0.2349\n",
      "Epoch 14/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2627 - val_loss: 0.2166\n",
      "Epoch 15/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2498 - val_loss: 0.2241\n",
      "Epoch 16/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2504 - val_loss: 0.2149\n",
      "Epoch 17/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2500 - val_loss: 0.2166\n",
      "Epoch 18/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2463 - val_loss: 0.2123\n",
      "Epoch 19/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2378 - val_loss: 0.2023\n",
      "Epoch 20/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2349 - val_loss: 0.2078\n",
      "Epoch 21/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2343 - val_loss: 0.2081\n",
      "Epoch 22/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2307 - val_loss: 0.2023\n",
      "Epoch 23/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2294 - val_loss: 0.1983\n",
      "Epoch 24/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2216 - val_loss: 0.1982\n",
      "Epoch 25/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2220 - val_loss: 0.1981\n",
      "Epoch 26/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2239 - val_loss: 0.1927\n",
      "Epoch 27/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2243 - val_loss: 0.2006\n",
      "Epoch 28/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2174 - val_loss: 0.1913\n",
      "Epoch 29/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2209 - val_loss: 0.1926\n",
      "Epoch 30/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2184 - val_loss: 0.1916\n",
      "Epoch 31/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2122 - val_loss: 0.1883\n",
      "Epoch 32/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2070 - val_loss: 0.1851\n",
      "Epoch 33/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2106 - val_loss: 0.1879\n",
      "Epoch 34/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2119 - val_loss: 0.1942\n",
      "Epoch 35/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2050 - val_loss: 0.1873\n",
      "Epoch 36/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1995 - val_loss: 0.1791\n",
      "Epoch 37/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1958 - val_loss: 0.1773\n",
      "Epoch 38/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.2052 - val_loss: 0.1840\n",
      "Epoch 39/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1999 - val_loss: 0.1818\n",
      "Epoch 40/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1988 - val_loss: 0.1767\n",
      "Epoch 41/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1972 - val_loss: 0.1823\n",
      "Epoch 42/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1928 - val_loss: 0.1770\n",
      "Epoch 43/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1952 - val_loss: 0.1914\n",
      "Epoch 44/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1996 - val_loss: 0.1925\n",
      "Epoch 45/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1940 - val_loss: 0.1765\n",
      "Epoch 46/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1946 - val_loss: 0.1771\n",
      "Epoch 47/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1878 - val_loss: 0.1734\n",
      "Epoch 48/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1877 - val_loss: 0.1641\n",
      "Epoch 49/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1914 - val_loss: 0.1745\n",
      "Epoch 50/50\n",
      "54000/54000 [==============================] - 1s - loss: 0.1890 - val_loss: 0.1716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x123db1c10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Train the model on the training data using .fit\n",
    "'''\n",
    "model.fit(X_train, \n",
    "          y_train_ohe, \n",
    "          nb_epoch = 50, # train over 50 cycles\n",
    "          batch_size = 300, # 300 training samples per batch\n",
    "          verbose = 1, \n",
    "          validation_split = 0.1, # reserve 10% of training data after each epoch,to check for overfitting during training\n",
    "          show_accuracy = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Make predictions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('First 3 predictions: ', array([5, 0, 4]))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Use predict_classes\n",
    "'''\n",
    "y_train_pred = model.predict_classes(X_train, \n",
    "                                     verbose=0)\n",
    "print('First 3 predictions: ', y_train_pred[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Performance evaluation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 ..., 5 6 8]\n",
      "[5 0 4 ..., 5 6 8]\n",
      "60000\n",
      "Training accuracy: 94.60%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Accuracy on training set\n",
    "'''\n",
    "print y_train_pred\n",
    "print y_train\n",
    "print X_train.shape[0]\n",
    "from __future__ import division\n",
    "\n",
    "# Computer accuracy\n",
    "train_acc = np.sum(y_train == y_train_pred, axis=0) / X_train.shape[0]\n",
    "print('Training accuracy: %.2f%%' % (train_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 93.95%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Accuracy on testing set\n",
    "'''\n",
    "y_test_pred = model.predict_classes(X_test, verbose=0)\n",
    "test_acc = np.sum(y_test == y_test_pred, axis=0) / X_test.shape[0]\n",
    "print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
