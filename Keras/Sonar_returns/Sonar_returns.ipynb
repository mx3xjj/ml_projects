{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: Classify whether sonar chirps are being reflected from rocks or metal cylinders #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the UCI Sonar dataset - https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/\n",
    "\n",
    "This is binary classification problem.\n",
    "\n",
    "Features of the dataset:\n",
    "\n",
    "* All features(60 features total) have continuous values\n",
    "* Outout variable is: M for mine, R for rock\n",
    "* Total 208 observations\n",
    "* This dataset is used as a standard benchmark problem. Our aim is to get a classification accuracy greater than 84%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Standardization and pipelines ##\n",
    "\n",
    "In our first approach we do the following:\n",
    "\n",
    "* Load dataset\n",
    "* Split features and target variable\n",
    "* One hot encode the target\n",
    "* Define NN \n",
    "* Standardize the dataset such that the mean value for each attribute is 0 and the standard deviation is 1.\n",
    "\n",
    "** Standardization and pipelines **\n",
    "\n",
    "* Rather than performing standardization on the entire datase, we run the standardization procedure within the pass of a cross validation run. \n",
    "* And then we use the trained standardized instance to preapre the unseen test fold. \n",
    "* This makes standardization a step in model preparation in the cross validation process and prevents the algorithm from having knowledge of the unseen data during the evaluation. \n",
    "* We achieve this using pipeline(), which is a wrapper that executes one or more models within a pass of the cross validation procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Necessary Imports\n",
    "'''\n",
    "# Binary Classification with Sonar Dataset: Standardized\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Load dataset\n",
    "'''\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "'''\n",
    "Split features and target variables\n",
    "'''\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "'''\n",
    "One hot encode the target variable\n",
    "'''\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Define NN\n",
    "'''\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # 60 input features\n",
    "    model.add(Dense(60, \n",
    "                    input_dim=60, \n",
    "                    init='normal', \n",
    "                    activation='relu'))\n",
    "    # 1 output target\n",
    "    model.add(Dense(1, \n",
    "                    init='normal', \n",
    "                    activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate baseline model with standardized dataset. Use Pipeline() to run the standardization during the cross \n",
    "validation.\n",
    "'''\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# List to be fed into Pipeline()\n",
    "estimators = []\n",
    "\n",
    "# Standardize the dataset\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', \n",
    "                   KerasClassifier(build_fn = create_baseline, \n",
    "                                   nb_epoch = 100, \n",
    "                                   batch_size = 5, \n",
    "                                   verbose = 0)))\n",
    "\n",
    "# Feed estimators into Pipeline\n",
    "pipeline = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 84.16% (5.60%)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "k-fold cross validation \n",
    "'''\n",
    "kfold = StratifiedKFold(n_splits=10, \n",
    "                        shuffle=True, \n",
    "                        random_state=seed)\n",
    "'''\n",
    "Get results\n",
    "'''\n",
    "results = cross_val_score(pipeline, \n",
    "                          X, \n",
    "                          encoded_Y, \n",
    "                          cv = kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Optimizing the network topology by making it smaller ##\n",
    "\n",
    "Here we will optimize the network by training the model on a smaller network.\n",
    "\n",
    "* We force feature extraction by having the hidden layer have only 30 neurons and not 60.\n",
    "* This forces the network to choose the most important features to be fed forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller: 82.21% (4.28%)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Same as Part 1\n",
    "'''\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "'''\n",
    "Change in NN structure made here. \n",
    "'''\n",
    "# smaller model\n",
    "def create_smaller():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    '''\n",
    "    Change made here: Hidden layer with 30 neurons instead of 60. \n",
    "    '''\n",
    "    model.add(Dense(30, \n",
    "                    input_dim=60, \n",
    "                    init='normal', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(1, \n",
    "                    init='normal', \n",
    "                    activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Standardize and pipeline the estimators\n",
    "'''\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', \n",
    "                   StandardScaler()))\n",
    "estimators.append(('mlp', \n",
    "                   KerasClassifier(build_fn=create_smaller, \n",
    "                                   nb_epoch=100, \n",
    "                                   batch_size=5, \n",
    "                                   verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "\n",
    "'''\n",
    "Stratified split \n",
    "'''\n",
    "kfold = StratifiedKFold(n_splits=10, \n",
    "                        shuffle=True, \n",
    "                        random_state=seed)\n",
    "\n",
    "'''\n",
    "Get cross validated scores\n",
    "'''\n",
    "results = cross_val_score(pipeline, \n",
    "                          X, \n",
    "                          encoded_Y, \n",
    "                          cv=kfold)\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimizing network topology by making it bigger ##\n",
    "\n",
    "Here we will add an extra hidden layer with 30 neurons. The idea here is as follows:\n",
    "\n",
    "* Give the model the opportunity to model all input variables before being bottlenecked  and forced to halve the representational capacity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller: 84.57% (4.78%)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Same as Part 2\n",
    "'''\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "'''\n",
    "Change in NN structure made here. \n",
    "'''\n",
    "# smaller model\n",
    "def create_smaller():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    '''\n",
    "    Change made here: Extra hidden layer with 30 neurons\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(60, \n",
    "                    input_dim=60, \n",
    "                    init='normal', \n",
    "                    activation='relu'))\n",
    "    # Extra hidden layer\n",
    "    model.add(Dense(30, \n",
    "                    init='normal', \n",
    "                    activation='relu'))\n",
    "    # Output layer\n",
    "    model.add(Dense(1, \n",
    "                    init='normal', \n",
    "                    activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "'''\n",
    "Standardize and pipeline the estimators\n",
    "'''\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', \n",
    "                   StandardScaler()))\n",
    "estimators.append(('mlp', \n",
    "                   KerasClassifier(build_fn=create_smaller, \n",
    "                                   nb_epoch=100, \n",
    "                                   batch_size=5, \n",
    "                                   verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "\n",
    "'''\n",
    "Stratified split \n",
    "'''\n",
    "kfold = StratifiedKFold(n_splits=10, \n",
    "                        shuffle=True, \n",
    "                        random_state=seed)\n",
    "\n",
    "'''\n",
    "Get cross validated scores\n",
    "'''\n",
    "results = cross_val_score(pipeline, \n",
    "                          X, \n",
    "                          encoded_Y, \n",
    "                          cv=kfold)\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
