{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture #\n",
    "\n",
    "* NN consists of 3 layers: input, hidden and output layers.\n",
    "* A n/w where the flow of learning is passed forward all the way to the outputs in one pass is called the feed forward NN.\n",
    "* The strength of the connections is expressed as weights and these are passed into the activation function. The goal of the activation function is to convert the input to the output.\n",
    "\n",
    "** Output range **\n",
    "\n",
    "This is the range of the actual output.\n",
    "\n",
    "** Active range **\n",
    "\n",
    "It is the range where the gradient has the most variance in the final weight updates. Outside this range, the gradient is near zero and does not add to the parameter updates during learning. This problem of a close-to-zero gradient is called the ** vanishing gradient problem ** and is solved by the **ReLU(Rectified Linear Unit)** activation function.\n",
    "\n",
    "Activation fucntion options:\n",
    "\n",
    "* sigmoid\n",
    "    - Active range: [sqrt(3), sqrt(3)]\n",
    "    - Output range: (0,1)\n",
    "* tanh(rescaled version of sigmoid function)\n",
    "    - Active range: [-2,2]\n",
    "    - Output range: (-1,1)\n",
    "* ReLU(Rectified Linear Unit) - best suited to deal with large NNs and the vanishing gradient problem.\n",
    "    - Active range: [0,infinity]\n",
    "\n",
    "## Softmax for classification ##\n",
    "\n",
    "Softmax Regression (synonyms: Multinomial Logistic, Maximum Entropy Classifier, or just Multi-class Logistic Regression) is a generalization of logistic regression that we can use for **multi-class classification** (under the assumption that the classes are mutually exclusive). In contrast, we use the (standard) Logistic Regression model in binary classification tasks.\n",
    "The outputs of the last hidden layer need to converted to probability outputs so that  final class prediction can be made. If the output is significantly lower than the MAX of all output values, softmax converts that value to a near 0 value.\n",
    "\n",
    "## Forward Propagation ##\n",
    "\n",
    "It occurs as follows:\n",
    "\n",
    "* Dot product on the inputs with the weights between the first and second layer and then transforming the result with the activation function.\n",
    "* Dot product on the outputs of the first hidden layer with the weights between the second and third layer and transforming the result with the activation fucntion.\n",
    "* Multiply the final vector with the activation fucntion(softmax for classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Implementation of Forward propagation\n",
    "'''\n",
    "import numpy as np\n",
    "import math\n",
    "b1 = 0 #bias unit 1\n",
    "b2 = 0 #bias unit 2\n",
    "\n",
    "# Activation function - sigmoid function\n",
    "def sigmoid(x):      \n",
    "    return 1 /(1+(math.e**-x))\n",
    "\n",
    "# Softmax function applied at output layer for classification\n",
    "def softmax(x):     \n",
    "    l_exp = np.exp(x)\n",
    "    sm = l_exp/np.sum(l_exp, axis=0)\n",
    "    return sm\n",
    "    \n",
    "# Test Input dataset with 3 features\n",
    "X = np.array([  [.35,.21,.33],\n",
    "            \t[.2,.4,.3],\n",
    "            \t[.4,.34,.5],\n",
    "            \t[.18,.21,16] ])\n",
    "\n",
    "# Training set size\n",
    "len_X = len(X) \n",
    "\n",
    "# Input layer dimensionality\n",
    "input_dim = 3 \n",
    "\n",
    "# Output layer dimensionality\n",
    "output_dim = 1 \n",
    "hidden_units=4\n",
    "  \n",
    "np.random.seed(22)\n",
    "\n",
    "# Create random weight vectors between input and hidden layer\n",
    "theta0 = 2 * np.random.random((input_dim, hidden_units))\n",
    "\n",
    "# Create random weight vectors between hidden and output layer\n",
    "theta1 = 2 * np.random.random((hidden_units, output_dim))\n",
    "\n",
    "\n",
    "'''\n",
    "Forward propagation:\n",
    "\n",
    "- Dot product on the inputs with the weights between the first and second layer \n",
    "  and then transforming the result with the activation function.\n",
    "  \n",
    "- Dot product on the outputs of the first hidden layer with the weights between the second and third layer \n",
    "  and transforming the result with the activation function.\n",
    "  \n",
    "- Apply softmax to output of final layer.\n",
    "'''\n",
    "d1 = X.dot(theta0) + b1\n",
    "l1 = sigmoid(d1)\n",
    "l2 = l1.dot(theta1) + b2\n",
    "\n",
    "# Apply softmax to the output of the final layer\n",
    "output = softmax(l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
