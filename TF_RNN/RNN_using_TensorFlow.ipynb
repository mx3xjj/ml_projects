{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://classroom.udacity.com/nanodegrees/nd101/parts/2a9dba0b-28eb-4b0e-acfa-bdcf35680d90/modules/a8b3293a-bb3e-4247-927e-739fbf3f5515/lessons/d74891dc-49c8-4569-9cab-6d508083e841/concepts/36be59a4-aca3-4281-9c47-2b695de95a46\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Loading the text data\n",
    "'''\n",
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "\n",
    "# get the character vocabulary set\n",
    "vocab = set(text)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 21,\n",
       " ' ': 23,\n",
       " '!': 11,\n",
       " '\"': 29,\n",
       " '$': 26,\n",
       " '%': 67,\n",
       " '&': 3,\n",
       " \"'\": 0,\n",
       " '(': 20,\n",
       " ')': 56,\n",
       " '*': 34,\n",
       " ',': 77,\n",
       " '-': 4,\n",
       " '.': 43,\n",
       " '/': 62,\n",
       " '0': 60,\n",
       " '1': 76,\n",
       " '2': 72,\n",
       " '3': 7,\n",
       " '4': 73,\n",
       " '5': 2,\n",
       " '6': 32,\n",
       " '7': 51,\n",
       " '8': 78,\n",
       " '9': 30,\n",
       " ':': 14,\n",
       " ';': 44,\n",
       " '?': 24,\n",
       " '@': 31,\n",
       " 'A': 64,\n",
       " 'B': 16,\n",
       " 'C': 57,\n",
       " 'D': 70,\n",
       " 'E': 17,\n",
       " 'F': 25,\n",
       " 'G': 28,\n",
       " 'H': 39,\n",
       " 'I': 49,\n",
       " 'J': 40,\n",
       " 'K': 33,\n",
       " 'L': 1,\n",
       " 'M': 9,\n",
       " 'N': 48,\n",
       " 'O': 36,\n",
       " 'P': 65,\n",
       " 'Q': 66,\n",
       " 'R': 79,\n",
       " 'S': 58,\n",
       " 'T': 6,\n",
       " 'U': 27,\n",
       " 'V': 8,\n",
       " 'W': 35,\n",
       " 'X': 75,\n",
       " 'Y': 68,\n",
       " 'Z': 59,\n",
       " '_': 71,\n",
       " '`': 81,\n",
       " 'a': 80,\n",
       " 'b': 82,\n",
       " 'c': 50,\n",
       " 'd': 55,\n",
       " 'e': 18,\n",
       " 'f': 41,\n",
       " 'g': 54,\n",
       " 'h': 45,\n",
       " 'i': 19,\n",
       " 'j': 38,\n",
       " 'k': 10,\n",
       " 'l': 53,\n",
       " 'm': 42,\n",
       " 'n': 61,\n",
       " 'o': 74,\n",
       " 'p': 5,\n",
       " 'q': 47,\n",
       " 'r': 12,\n",
       " 's': 15,\n",
       " 't': 69,\n",
       " 'u': 52,\n",
       " 'v': 63,\n",
       " 'w': 13,\n",
       " 'x': 37,\n",
       " 'y': 46,\n",
       " 'z': 22}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character : integer dictionary\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: \"'\",\n",
       " 1: 'L',\n",
       " 2: '5',\n",
       " 3: '&',\n",
       " 4: '-',\n",
       " 5: 'p',\n",
       " 6: 'T',\n",
       " 7: '3',\n",
       " 8: 'V',\n",
       " 9: 'M',\n",
       " 10: 'k',\n",
       " 11: '!',\n",
       " 12: 'r',\n",
       " 13: 'w',\n",
       " 14: ':',\n",
       " 15: 's',\n",
       " 16: 'B',\n",
       " 17: 'E',\n",
       " 18: 'e',\n",
       " 19: 'i',\n",
       " 20: '(',\n",
       " 21: '\\n',\n",
       " 22: 'z',\n",
       " 23: ' ',\n",
       " 24: '?',\n",
       " 25: 'F',\n",
       " 26: '$',\n",
       " 27: 'U',\n",
       " 28: 'G',\n",
       " 29: '\"',\n",
       " 30: '9',\n",
       " 31: '@',\n",
       " 32: '6',\n",
       " 33: 'K',\n",
       " 34: '*',\n",
       " 35: 'W',\n",
       " 36: 'O',\n",
       " 37: 'x',\n",
       " 38: 'j',\n",
       " 39: 'H',\n",
       " 40: 'J',\n",
       " 41: 'f',\n",
       " 42: 'm',\n",
       " 43: '.',\n",
       " 44: ';',\n",
       " 45: 'h',\n",
       " 46: 'y',\n",
       " 47: 'q',\n",
       " 48: 'N',\n",
       " 49: 'I',\n",
       " 50: 'c',\n",
       " 51: '7',\n",
       " 52: 'u',\n",
       " 53: 'l',\n",
       " 54: 'g',\n",
       " 55: 'd',\n",
       " 56: ')',\n",
       " 57: 'C',\n",
       " 58: 'S',\n",
       " 59: 'Z',\n",
       " 60: '0',\n",
       " 61: 'n',\n",
       " 62: '/',\n",
       " 63: 'v',\n",
       " 64: 'A',\n",
       " 65: 'P',\n",
       " 66: 'Q',\n",
       " 67: '%',\n",
       " 68: 'Y',\n",
       " 69: 't',\n",
       " 70: 'D',\n",
       " 71: '_',\n",
       " 72: '2',\n",
       " 73: '4',\n",
       " 74: 'o',\n",
       " 75: 'X',\n",
       " 76: '1',\n",
       " 77: ',',\n",
       " 78: '8',\n",
       " 79: 'R',\n",
       " 80: 'a',\n",
       " 81: '`',\n",
       " 82: 'b'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# integer : character dictionary\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 45, 80, ..., 10, 15, 43], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "convert characters to integers using the charater:integer dictionary\n",
    "'''\n",
    "chars = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 100 chars in text\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([57, 45, 80,  5, 69, 18, 12, 23, 76, 21, 21, 21, 39, 80,  5,  5, 46,\n",
       "       23, 41, 80, 42, 19, 53, 19, 18, 15, 23, 80, 12, 18, 23, 80, 53, 53,\n",
       "       23, 80, 53, 19, 10, 18, 44, 23, 18, 63, 18, 12, 46, 23, 52, 61, 45,\n",
       "       80,  5,  5, 46, 23, 41, 80, 42, 19, 53, 46, 23, 19, 15, 23, 52, 61,\n",
       "       45, 80,  5,  5, 46, 23, 19, 61, 23, 19, 69, 15, 23, 74, 13, 61, 21,\n",
       "       13, 80, 46, 43, 21, 21, 17, 63, 18, 12, 46, 69, 45, 19, 61], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 100 characters converted to integers\n",
    "chars[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1985222,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of the chars list. This will give the total number of numbers in the list\n",
    "chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(chars, batch_size, num_steps, split_frac=0.9):\n",
    "    \"\"\" \n",
    "    Split character data into training and validation sets, inputs and targets for each set.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    chars: character array\n",
    "    batch_size: Size of examples in each of batch\n",
    "    num_steps: Number of sequence steps to keep in the input and pass to the network\n",
    "    split_frac: Fraction of batches to keep in the training set\n",
    "    \n",
    "    \n",
    "    Returns train_x, train_y, val_x, val_y\n",
    "    \"\"\"\n",
    "    # num_steps: Sequence length of characters we are going to pass into the network.\n",
    "    #            The longer the sequence is, the further back it can look for correlations between characters.   \n",
    "    # Eg: 100 numbers in the chars list, batch_size = 10, num_steps = 1\n",
    "    # Eg: slice_size = 10 * 1 = 10\n",
    "    slice_size = batch_size * num_steps\n",
    "    # Get the total number of batches we would get from the data \n",
    "    # by dividing total number of numbers(words) by the size of each slice(words in each slice)\n",
    "    # Eg: n_batches = 100/10 = 10 batches\n",
    "    n_batches = int(len(chars) / slice_size)\n",
    "    \n",
    "    # Drop the last few characters to make only full batches\n",
    "    # Eg: x = chars[:10]\n",
    "    x = chars[: n_batches*slice_size]\n",
    "    # Our target variable is one value ahead of the feature \n",
    "    # Eg: y = chars[1:11]\n",
    "    y = chars[1: n_batches*slice_size + 1]\n",
    "    \n",
    "    # Split the data into batch_size slices, then stack them into a 2D matrix \n",
    "    # This is vertically stacked data.\n",
    "    # rows = batch_size, columns = rest of the data. width is the number of steps in the sequence\n",
    "    x = np.stack(np.split(x, batch_size))\n",
    "    y = np.stack(np.split(y, batch_size))\n",
    "    # Now x and y are arrays with dimensions (batch_size x n_batches*num_steps)\n",
    "    \n",
    "    # Split into training and validation sets, keep the first split_frac batches for training\n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    \n",
    "    # training set\n",
    "    train_x, train_y = x[:, :split_idx*num_steps], y[:, :split_idx*num_steps]\n",
    "    # validation set\n",
    "    val_x, val_y = x[:, split_idx*num_steps:], y[:, split_idx*num_steps:]\n",
    "    \n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 178650)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size = 10, num_steps = 50\n",
    "train_x, train_y, val_x, val_y = split_data(chars, 10, 50)\n",
    "# rows = batch_size\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Sample showing the working of numpy stack\n",
    "'''\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 3, 4])\n",
    "np.stack((a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.]\n",
      "[array([ 0.,  1.,  2.]), array([ 3.,  4.,  5.]), array([ 6.,  7.,  8.])]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Sample showing the working of numpy split - splits array into multiple sub arrays\n",
    "'''\n",
    "c = np.arange(9.0)\n",
    "print(c)\n",
    "print(np.split(c, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57, 45, 80,  5, 69, 18, 12, 23, 76, 21, 21, 21, 39, 80,  5],\n",
       "       [23, 80, 42, 23, 61, 74, 69, 23, 54, 74, 19, 61, 54, 23, 69],\n",
       "       [63, 19, 61, 43, 21, 21, 29, 68, 18, 15, 77, 23, 19, 69,  0],\n",
       "       [61, 23, 55, 52, 12, 19, 61, 54, 23, 45, 19, 15, 23, 50, 74],\n",
       "       [23, 19, 69, 23, 19, 15, 77, 23, 15, 19, 12, 11, 29, 23, 15],\n",
       "       [23, 49, 69, 23, 13, 80, 15, 21, 74, 61, 53, 46, 23, 13, 45],\n",
       "       [45, 18, 61, 23, 50, 74, 42, 18, 23, 41, 74, 12, 23, 42, 18],\n",
       "       [44, 23, 82, 52, 69, 23, 61, 74, 13, 23, 15, 45, 18, 23, 13],\n",
       "       [69, 23, 19, 15, 61,  0, 69, 43, 23,  6, 45, 18, 46,  0, 12],\n",
       "       [23, 15, 80, 19, 55, 23, 69, 74, 23, 45, 18, 12, 15, 18, 53]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returning all batches for first 15 steps\n",
    "train_x[:, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function to grab batches out of the arrays made by split_data. \n",
    "Here each batch will be a sliding window on these arrays with size batch_size X num_steps. \n",
    "For example, if we want our network to train on a sequence of 100 characters, \n",
    "num_steps = 100. For the next batch, we'll shift this window the next sequence of num_steps \n",
    "characters. In this way we can feed batches to the network and the cell states will \n",
    "continue through on each batch.\n",
    "'''\n",
    "def get_batch(arrs, num_steps):\n",
    "    \n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_steps)\n",
    "    \n",
    "    for b in range(n_batches):\n",
    "        yield [x[ : , b*num_steps : (b+1)*num_steps] for x in arrs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: RNN model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(num_classes, \n",
    "              batch_size=50, \n",
    "              num_steps=50, \n",
    "              lstm_size=128, \n",
    "              num_layers=2,\n",
    "              learning_rate=0.001, \n",
    "              grad_clip=5, \n",
    "              sampling=False):\n",
    "    \n",
    "    # When we're using this network for sampling later, we'll be passing in\n",
    "    # one character at a time, so providing an option for that\n",
    "    if sampling == True:\n",
    "        batch_size, num_steps = 1, 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Declare placeholders we'll feed into the graph\n",
    "    inputs = tf.placeholder(tf.int32, \n",
    "                            [batch_size, num_steps], \n",
    "                            name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, \n",
    "                             [batch_size, num_steps], \n",
    "                             name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, \n",
    "                               name='keep_prob')\n",
    "    \n",
    "    # One-hot encoding the input and target characters\n",
    "    x_one_hot = tf.one_hot(inputs, \n",
    "                           num_classes)\n",
    "    y_one_hot = tf.one_hot(targets, \n",
    "                           num_classes)\n",
    "    \n",
    "    ### Build the RNN layers\n",
    "    # Use a basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                         output_keep_prob = keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    # Setting the cell state to 0\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    ### Run the data through the RNN layers\n",
    "    # This makes a list where each element is one step in the sequence\n",
    "    rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "    \n",
    "    # Run each sequence step through the RNN and collect the outputs\n",
    "    # outputs is the outputs of the hidden layer for each step in the sequence\n",
    "    outputs, state = tf.contrib.rnn.static_rnn(cell, \n",
    "                                               rnn_inputs, \n",
    "                                               initial_state=initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    # Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "    seq_output = tf.concat(outputs, \n",
    "                           axis=1)\n",
    "    # Each row is one output. lstm_size is the number of hidden layers in our cells\n",
    "    # The number of columns or width of output is the number of hidden layers in the cells - lstm_size\n",
    "    output = tf.reshape(seq_output, \n",
    "                        [-1, lstm_size])\n",
    "    \n",
    "    # Now connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('softmax'):\n",
    "        # Weights\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), \n",
    "                                                    stddev=0.1))\n",
    "        # Biases\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "        \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and batch\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    \n",
    "    # Use softmax to get the probabilities for predicted characters\n",
    "    preds = tf.nn.softmax(logits, \n",
    "                          name='predictions')\n",
    "    \n",
    "    # Reshape the targets to match the logits\n",
    "    y_reshaped = tf.reshape(y_one_hot, \n",
    "                            [-1, num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits = logits, \n",
    "                                                   labels = y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    # Optimizer for training, using gradient clipping to control exploding gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), \n",
    "                                      grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    # Export the nodes\n",
    "    export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                    'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "    Graph = namedtuple('Graph', \n",
    "                       export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "batch_size - Number of sequences running through the network in one pass.\n",
    "\n",
    "num_steps - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "\n",
    "lstm_size - The number of units in the hidden layers.\n",
    "\n",
    "num_layers - Number of hidden LSTM layers to use\n",
    "\n",
    "learning_rate - Learning rate for training\n",
    "\n",
    "keep_prob - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "'''\n",
    "batch_size = 100\n",
    "num_steps = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1  Iteration 1/178 Training loss: 4.4206 11.8804 sec/batch\n",
      "Epoch 1/1  Iteration 2/178 Training loss: 4.3760 8.6766 sec/batch\n",
      "Epoch 1/1  Iteration 3/178 Training loss: 4.2257 7.5384 sec/batch\n",
      "Epoch 1/1  Iteration 4/178 Training loss: 4.5049 9.2701 sec/batch\n",
      "Epoch 1/1  Iteration 5/178 Training loss: 4.4487 7.6899 sec/batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-90fc45608652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n\u001b[0;32m---> 36\u001b[0;31m                                                  feed_dict=feed)\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adarshnair/graphlab/anaconda/envs/py3k/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adarshnair/graphlab/anaconda/envs/py3k/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adarshnair/graphlab/anaconda/envs/py3k/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/adarshnair/graphlab/anaconda/envs/py3k/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adarshnair/graphlab/anaconda/envs/py3k/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, val_x, val_y = split_data(chars, batch_size, num_steps)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_steps=num_steps,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_steps)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_steps), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            \n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([val_x, val_y], num_steps):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = \"checkpoints/____.ckpt\"\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
