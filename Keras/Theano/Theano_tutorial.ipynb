{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano #\n",
    "\n",
    "Theano is built around tensors to evaluate symbolic mathematical expressions. \n",
    "\n",
    "* Scalar: rank-0 tensor\n",
    "* Vector: rank-1 tensor\n",
    "* Matrix: rank-2 tensor\n",
    "\n",
    "Code in Theano follows 3 steps:\n",
    "\n",
    "1. Define the symbols(variable objects)\n",
    "2. Compile code.\n",
    "3. Execute code.\n",
    "\n",
    "We have to choose if we want to use 64 or 32 bit integers/floats, which affects the performance of our code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using theano.function ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(2.5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "'''\n",
    "Compute a net input z of a sample point x in a one dimensional dataset with weight w1 and bias w0.\n",
    "\n",
    "z = x1 * w1 + w0\n",
    "'''\n",
    "\n",
    "# initialize the variables\n",
    "x1 = T.scalar()\n",
    "w1 = T.scalar()\n",
    "w0 = T.scalar()\n",
    "\n",
    "# define equation\n",
    "z1 = w1 * x1 + w0\n",
    "\n",
    "# compile code\n",
    "net_input = theano.function(inputs = [w1, x1, w0], \n",
    "                            outputs = z1)\n",
    "\n",
    "# execute code\n",
    "net_input(2.0, 1.0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Defualt dtype is float64 which runs on the CPU. It can be changed to float32 which runs on the GPU as follows:\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "'''\n",
    "print(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the float type globally, execute\n",
    "\n",
    "`export THEANO_FLAGS=floatX=float32 `\n",
    "\n",
    "in your bash shell. Or execute Python script as\n",
    "\n",
    "`THEANO_FLAGS=floatX=float32 python your_script.py`\n",
    "\n",
    "Running Theano on GPU(s). For prerequisites, please see: http://deeplearning.net/software/theano/tutorial/using_gpu.html\n",
    "\n",
    "Note that float32 is recommended for GPUs; float64 on GPUs is currently still relatively slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check where the theano code is running.\n",
    "'''\n",
    "print(theano.config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Apply setting only to a particular python script - you can run a Python script on CPU via: ** \n",
    "\n",
    "`THEANO_FLAGS=device=cpu,floatX=float64 python your_script.py`\n",
    "\n",
    "** Apply setting only to a particular python script to be run on the GPU via ** \n",
    "\n",
    "`THEANO_FLAGS=device=gpu,floatX=float32 python your_script.py`\n",
    "\n",
    "** It may also be convenient to create a .theanorc file in your home directory to make those configurations permanent. For example, to always use float32, execute ** \n",
    "\n",
    "`echo -e \"\\n[global]\\nfloatX=float32\\n\" >> ~/.theanorc`\n",
    "\n",
    "** Or, create a .theanorc file manually with the following contents ** \n",
    "\n",
    "`[global]\n",
    "floatX = float32\n",
    "device = gpu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using theano to work with arrays ##\n",
    "\n",
    "* fmatrix\n",
    "* dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Column sum:', array([ 2.,  4.,  6.]))\n",
      "('Column sum:', array([ 2.,  4.,  6.]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# initialize variables for theano function.\n",
    "# if you are running Theano on 64 bit mode, \n",
    "# you need to use dmatrix instead of fmatrix\n",
    "x = T.dmatrix(name='x')\n",
    "x_sum = T.sum(x, axis=0)\n",
    "\n",
    "# compile code\n",
    "calc_sum = theano.function(inputs = [x], \n",
    "                           outputs = x_sum)\n",
    "\n",
    "# execute (Python list)\n",
    "ary = [[1, 2, 3], [1, 2, 3]]\n",
    "print('Column sum:', calc_sum(ary))\n",
    "\n",
    "# execute (NumPy array)\n",
    "ary = np.array([[1, 2, 3], [1, 2, 3]], \n",
    "               dtype = theano.config.floatX)\n",
    "print('Column sum:', calc_sum(ary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the shared variable ##\n",
    "\n",
    "http://deeplearning.net/software/theano/tutorial/aliasing.html\n",
    "\n",
    "The main idea is that there is a pool of memory managed by Theano, and Theano tracks changes to values in that pool. Theano spreads memory space across multiple devices(CPUs, GPUs), to track changes in the memory space it aliases the respective buffers. \n",
    "\n",
    "* The shared variable allows us to spread large objects like arrays, and grant multiple functions read and write access so that we can perform updates on those objects after compilation. \n",
    "* Theano manages its own memory space, which typically does not overlap with the memory of normal Python variables that non-Theano code creates.\n",
    "* Theano functions only modify buffers that are in Theano’s memory space.\n",
    "* Theano’s memory space includes the buffers allocated to store shared variables and the temporaries used to evaluate functions.\n",
    "* Physically, Theano’s memory space may be spread across the host, a GPU device(s), and in the future may even include objects on a remote machine.\n",
    "* The memory allocated for a shared variable buffer is unique: it is never aliased to another shared variable.\n",
    "* Theano’s managed memory is constant while Theano functions are not running and Theano’s library code is not running.\n",
    "* The default behaviour of a function is to return user-space values for outputs, and to expect user-space values for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('z0:', array([[ 0.]]))\n",
      "('z1:', array([[ 6.]]))\n",
      "('z2:', array([[ 12.]]))\n",
      "('z3:', array([[ 18.]]))\n",
      "('z4:', array([[ 24.]]))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Define an update variable such that we want to update an array w by a value of 1.0 after each iteration of a for loop.\n",
    "'''\n",
    "\n",
    "# initialize variables\n",
    "x = T.dmatrix(name = 'x')\n",
    "w = theano.shared(np.asarray([[0.0, 0.0, 0.0]], \n",
    "                            dtype = theano.config.floatX))\n",
    "z = x.dot(w.T)\n",
    "update = [[w, w + 1.0]]\n",
    "\n",
    "# compile code\n",
    "net_input = theano.function(inputs = [x], \n",
    "                            updates = update, \n",
    "                            outputs = z)\n",
    "\n",
    "# execute code\n",
    "data = np.array([[1, 2, 3]], \n",
    "                dtype = theano.config.floatX)\n",
    "for i in range(5):\n",
    "    print('z%d:' % i, net_input(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the givens variable ##\n",
    "\n",
    "* We can insert values into the graph before compiling it. \n",
    "* Thus we can reduce the number of transfers from RAM over CPUs to GPUs to speed up learning algorithms that use shared variables.\n",
    "* If we use `inputs` in theano.function, data is transferred from the CPU to the GPU multiple times. \n",
    "* Using `givens` we can keep the dataset on the GPU if it fits into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('z:', array([[ 0.]]))\n",
      "('z:', array([[ 6.]]))\n",
      "('z:', array([[ 12.]]))\n",
      "('z:', array([[ 18.]]))\n",
      "('z:', array([[ 24.]]))\n"
     ]
    }
   ],
   "source": [
    "# initialize code(same as the previos node)\n",
    "data = np.array([[1, 2, 3]], \n",
    "                dtype=theano.config.floatX)\n",
    "x = T.dmatrix(name='x')\n",
    "w = theano.shared(np.asarray([[0.0, 0.0, 0.0]], \n",
    "                             dtype=theano.config.floatX))\n",
    "z = x.dot(w.T)\n",
    "update = [[w, w + 1.0]]\n",
    "\n",
    "# compile the code using givens\n",
    "net_input = theano.function(inputs = [], \n",
    "                            updates = update, \n",
    "                            givens = {x: data},\n",
    "                            outputs = z)\n",
    "\n",
    "# execute\n",
    "for i in range(5):\n",
    "    print('z:', net_input())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing activation functions for feedforward neural networks #\n",
    "\n",
    "* Sigmoid function = logistic function = negative log likelihood function\n",
    "* Linear functions are not very suitable for NN as we want to introduce non linearity to tackle complex problems.\n",
    "* Sigmoid/logisic functions can be problematic when we have highly -ve inputs, as the function will return values very close to 0. Therefore, the NN will learn very slowly and can get stuck in a local minima. \n",
    "* Hence, we use ** Hyperbolic tangents **."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic function/Sigmoid function implementation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(y=1|x) = 0.707\n"
     ]
    }
   ],
   "source": [
    "# note that first element (X[0] = 1) to denote bias unit\n",
    "X = np.array([[1, 1.4, 1.5]])\n",
    "# weights\n",
    "w = np.array([0.0, 0.2, 0.4])\n",
    "\n",
    "'''\n",
    "Dot product of inputs with their corresponding weights\n",
    "'''\n",
    "def net_input(X, w):\n",
    "    z = X.dot(w)\n",
    "    return z\n",
    "\n",
    "'''\n",
    "Feeding net input value(input dot weight) to Sigmoid function\n",
    "'''\n",
    "def logistic(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "'''\n",
    "Apply Sigmoid function as activation function\n",
    "'''\n",
    "def logistic_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return logistic(z)\n",
    "\n",
    "'''\n",
    "Probability that the sample belongs to the positive class\n",
    "'''\n",
    "print('P(y=1|x) = %.3f' % logistic_activation(X, w)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi layer perceptron with 3 hidden units using Sigmoid function ##\n",
    "\n",
    "MLP perceptron with 3 hidden units + 1 bias unit in the hidden unit. The output layer consists of 3 output units.\n",
    "\n",
    "Sigmoid function doesn't work as well when we have to compute multi class probabilities as we can see below, the probability percentages are 87%, 57% and 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Probabilities:\\n', array([[ 0.87653295],\n",
      "       [ 0.57688526],\n",
      "       [ 0.90114393]]))\n"
     ]
    }
   ],
   "source": [
    "# W : array, shape = [n_output_units, n_hidden_units+1]\n",
    "# Weight matrix for hidden layer -> output layer.\n",
    "# note that first column (W[:][0]) contains the bias units\n",
    "W = np.array([[1.1, 1.2, 1.3, 0.5],\n",
    "              [0.1, 0.2, 0.4, 0.1],\n",
    "              [0.2, 0.5, 2.1, 1.9]])\n",
    "\n",
    "# A : array, shape = [n_hidden+1, n_samples]\n",
    "# Activation of hidden layer.\n",
    "# note that first element (A[0][0] = 1) is for the bias units \n",
    "A = np.array([[1.0], \n",
    "              [0.1], \n",
    "              [0.3], \n",
    "              [0.7]])\n",
    "\n",
    "# Z : array, shape = [n_output_units, n_samples]\n",
    "# Net input of output layer.\n",
    "Z = W.dot(A) \n",
    "\n",
    "# Apply activation function\n",
    "y_probas = logistic(Z)\n",
    "\n",
    "'''\n",
    "Compute probabilities that sample belongs to which of the 3 classes. \n",
    "'''\n",
    "print('Probabilities:\\n', y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron using Softmax function for multi class classification ##\n",
    "\n",
    "* Softmax is a normalized logistic function\n",
    "* The probability of a particular sample with net input z, belongs to the i class, can be computed with a normalization term in the denominator that is the sum of all M linear functions.\n",
    "* The sum of all probabilities outputted by the softmax function will equal 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z))\n",
    "\n",
    "def softmax_activation(X, w):\n",
    "    z = net_input(X, w)\n",
    "    return softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Probabilities:\\n', array([[ 0.40386493],\n",
      "       [ 0.07756222],\n",
      "       [ 0.51857284]]))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Using softmax to compute multi class probabilities for the same input and weight vector example defined above.\n",
    "'''\n",
    "y_probas = softmax(Z)\n",
    "print('Probabilities:\\n', y_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP using a Hyperbolic tangent ##\n",
    "\n",
    "* Hyperbolic tangent is a rescaled version of the logistic function. \n",
    "* It's advantage over the logistic function is that it has a broader output spectrum(-1,1) which can improve the convergence of the back propagation algorithm. The sigmoid function output spectrum is (0,1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['tanh']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEbCAYAAACLGcAmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+z/Vqmkc6LOwDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103a2a590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Plotting output of tanh function vs sigmoid function\n",
    "'''\n",
    "def tanh(z):\n",
    "    e_p = np.exp(z) \n",
    "    e_m = np.exp(-z)\n",
    "    return (e_p - e_m) / (e_p + e_m)  \n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "z = np.arange(-5, 5, 0.005)\n",
    "log_act = logistic(z)\n",
    "tanh_act = tanh(z)\n",
    "\n",
    "# Can implement directly using numpy ---> np.tanh\n",
    "# from scipy.special import expit\n",
    "# log_act = expit(z)\n",
    "# tanh_act = np.tanh(z)\n",
    "\n",
    "plt.ylim([-1.5, 1.5])\n",
    "plt.xlabel('net input $z$')\n",
    "plt.ylabel('activation $\\phi(z)$')\n",
    "plt.axhline(1, color='black', linestyle='--')\n",
    "plt.axhline(0.5, color='black', linestyle='--')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.axhline(-1, color='black', linestyle='--')\n",
    "\n",
    "plt.plot(z, tanh_act, \n",
    "         linewidth=2, \n",
    "         color='black', \n",
    "         label='tanh')\n",
    "# plt.plot(z, log_act, \n",
    "#          linewidth=2, \n",
    "#          color='lightgreen', \n",
    "#          label='logistic')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('./figures/activation.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
